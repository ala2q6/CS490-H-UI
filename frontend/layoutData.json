{
  "TITLE" : [
    "Classification of Star-Galaxy Images Using a Neural Network",
    "A UMKC Hack-A-Roo Project",
    "Spring 2022",
    "Garver and Arbuckle"
  ],
  "IMAGE1" : "https://cdn.discordapp.com/attachments/948804016320679946/963003391116709948/IC745_H01_1821_1270_3.jpg",
  "SPACER1" : "",
  "About Our Project..." : [
    "The goal of this project is to use a neural network to classify astronomical objects. The astronomy dataset we are using provides us with a list of images that can be categorized into either ‘galaxy’ or ‘star’. The images have been gathered manually with a telescope and smaller 64x64 pixel cutouts have been made, centered on identifiable stars or galaxies. More information about the dataset and its collection method can be found on Kaggle [1]."
  ],
  "SPACER2" : "",
  "What We're Dealing With" : [
    "`Lorem` ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.",
    "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum."
  ],
  "SPACER3" :"",
  "About the Code..." : [
    "We decided to use a sequential model because we intended to have a linear stack of layers, so any benefit we would receive from a functional model would be forfeit.",
    "```\nmodel = Sequential()\nmodel.add(tf1.InputLayer(input_shape=inputShape))\nmodel.add(tf1.Rescaling(scale=1./255))\nmodel.add(tf1.Flatten())\nmodel.add(tf1.Dense(540, activation='relu'))\nmodel.add(tf1.Dense(1080, activation='relu'))\nmodel.add(tf1.BatchNormalization())\nmodel.add(tf1.Dense(640, activation='relu'))\nmodel.add(tf1.BatchNormalization())\nmodel.add(tf1.Dense(640, activation='relu'))\nmodel.add(tf1.BatchNormalization())\nmodel.add(tf1.Dense(340, activation='relu'))\nmodel.add(tf1.Dense(2, activation='softmax'))\n```",
    "We scale the input by 255 because that's the standard image pixel range. All hidden layers use the Rectified linear activation function because we thought this would most closely approximate the shape of the data.",
    "Below you can see our parameters for model compilation.",
    "```py\nmodel.compile(\n\tloss = 'categorical_crossentropy',\n\tmetrics = [tf.keras.metrics.Accuracy(), tf.keras.metrics.AUC()],\n\toptimizer = optimizers.Adam(learning_rate=0.005)\n) ```",
    "In the above example you can see that we are using the categorical cross entropy loss function. We did this because...",
    "We evaluate our model using the keras accuracy metric and the AUC (area under curve) metric. We use the AUC metric because it gives us a more accurate evaluation of binary classifiers.",
    "Lastly, we use the Adam optimizer because of its efficiency and ability to handle large datasets.",
    "Here, you can see what parameters we specified for fitting the model.",
    "```py\nhistory = model.fit(\n\txTrain,\n\tyTrain,\n\tbatch_size = 64,\n\tepochs = 20,\n\tverbose = 1,\n\tvalidation_data = (xValid, yValid)\n) ```",
    "The number of epochs has been limited and the batch size included to help prevent issues of over fitting."
  ],
  "SPACER4" : "",
  "The Results" : [
    "Unfortunately, we were unable to get our model working with a consistent accuracy above 20 percent before the deadline. Throughout our work during the Hackathon, we attempted both a traditional linear network and a convolutional neural network. The linear model gave us the provided accuracy, at its best getting to about 34 percent. Although we were able to get the convolutional network functioning, we were unable to get it to execute with any reasonable accuracy. We have a few ideas about where issues may have arisen. Foremost, we think that the difference between the individual data entries is small enough to warrant a much larger network, which is reinforced by our code references. Next, because we are analyzing images, we know a convolutional network will be more effective than a linear model. Finally, a better understanding of what the network is doing would allow us to more effectively choose our layers and increase accuracy."
  ],
  "SPACER5" : "",
  "Credits Go To..." : [
    "[1 Star-Galaxy Dataset](https://www.kaggle.com/datasets/divyansh22/dummy-astronomy-data)",
    "[2 Code Reference](https://www.kaggle.com/code/akzenith/auc-score-0-95-resnet-from-scratch)",
    "[3 Code Reference](https://www.kaggle.com/code/paulinecalut/star-and-galaxy-pauline)",
    "[4 TensorFlow Documentation](https://www.tensorflow.org/api_docs/)",
    "[5 Keras Documentation](https://keras.io/api/)",
    "[6 Dash Documentation](https://dash.plotly.com/introduction)"
  ]
}
